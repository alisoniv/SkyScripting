{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Notebook \n",
    "### Uses architecture in models/cnn_base.py\n",
    "\n",
    "Before running make sure to <br>\n",
    "    1. Create and activate python virutal environment in the skyscripting repo directory (NOT IN skyscript_app or skyscript_model)<br>\n",
    "    2. install jupyter and ipykernel with pip<br>\n",
    "    3. create ipykernel for that virtual environment<br>\n",
    "    4. set that new ipykernel as the running kernel for the notebook<br>\n",
    "<br>\n",
    "Sidenotes:<br>\n",
    "    Wavemix model does perform better but takes significantly longer to train and the benefit is marginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from models.cnn_base import CNNBaseModel\n",
    "\n",
    "# import wavemix\n",
    "# from wavemix.classification import WaveMix\n",
    "\n",
    "#Set the model weights save path\n",
    "CKPT_SAVE_PATH = \"model_ckpts/ocr_letters.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\n",
    "    \"0\": \"A\",\n",
    "    \"1\": \"B\",\n",
    "    \"2\": \"C\",\n",
    "    \"3\": \"D\",\n",
    "    \"4\": \"E\",\n",
    "    \"5\": \"F\",\n",
    "    \"6\": \"G\",\n",
    "    \"7\": \"H\",\n",
    "    \"8\": \"I\",\n",
    "    \"9\": \"J\",\n",
    "    \"10\": \"K\",\n",
    "    \"11\": \"L\",\n",
    "    \"12\": \"M\",\n",
    "    \"13\": \"N\",\n",
    "    \"14\": \"O\",\n",
    "    \"15\": \"P\",\n",
    "    \"16\": \"Q\",\n",
    "    \"17\": \"R\",\n",
    "    \"18\": \"S\",\n",
    "    \"19\": \"T\",\n",
    "    \"20\": \"U\",\n",
    "    \"21\": \"V\",\n",
    "    \"22\": \"W\",\n",
    "    \"23\": \"X\",\n",
    "    \"24\": \"Y\",\n",
    "    \"25\": \"Z\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hmofi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\functional.py:153: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Helper function to load MNIST-like .gz files\n",
    "def load_mnist_images(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        return data.reshape(-1, 28, 28)  # Shape: (num_samples, 28, 28)\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        return np.frombuffer(f.read(), np.uint8, offset=8)  # Shape: (num_samples,)\n",
    "\n",
    "# Define paths to the extracted files\n",
    "train_images_path = 'emnist_data/gzip/emnist-letters-train-images-idx3-ubyte.gz'\n",
    "train_labels_path = 'emnist_data/gzip/emnist-letters-train-labels-idx1-ubyte.gz'\n",
    "test_images_path = 'emnist_data/gzip/emnist-letters-test-images-idx3-ubyte.gz'\n",
    "test_labels_path = 'emnist_data/gzip/emnist-letters-test-labels-idx1-ubyte.gz'\n",
    "\n",
    "# Load the data\n",
    "train_images = load_mnist_images(train_images_path)\n",
    "train_labels = load_mnist_labels(train_labels_path)\n",
    "test_images = load_mnist_images(test_images_path)\n",
    "test_labels = load_mnist_labels(test_labels_path)\n",
    "\n",
    "# Create a custom PyTorch dataset\n",
    "class EMNISTDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformations\n",
    "        return image, label\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EMNISTDataset(train_images, train_labels, transform=transform)\n",
    "test_dataset = EMNISTDataset(test_images, test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate through the training data\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)  # Output: (batch_size, 1, 28, 28)\n",
    "    print(labels.shape)  # Output: (batch_size,)\n",
    "    break  # Just to see one batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load EMNIST balanced dataset\u001b[39;00m\n\u001b[0;32m      8\u001b[0m data_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 9\u001b[0m emnist_data \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mEMNIST(root\u001b[38;5;241m=\u001b[39m\u001b[43mdata\u001b[49m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mletters\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Create DataLoader\u001b[39;00m\n\u001b[0;32m     12\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load EMNIST balanced dataset\n",
    "data_root = './data'\n",
    "emnist_data = datasets.EMNIST(root=data, split='letters', train=True, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(emnist_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(emnist_data, batch_size=batch_size, shuffle=False )\n",
    "\n",
    "# Iterate through the data\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)  # Output: (batch_size, 1, 28, 28)\n",
    "    print(labels.shape)  # Output: (batch_size,)\n",
    "    break  # Just to see one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emnist_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(emnist_data\u001b[38;5;241m.\u001b[39mclasses)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m (1 extra class for NaN)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'emnist_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Number of classes {len(emnist_data.classes)} \\n (1 extra class for NaN)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(10, 8)) \n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].imshow(images[i].squeeze().T, cmap='gray')\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNBaseModel(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=576, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNNBaseModel()\n",
    "# model = WaveMix(\n",
    "#     num_classes= 27, \n",
    "#     depth= 16,\n",
    "#     mult= 2,\n",
    "#     ff_channel= 192,\n",
    "#     final_dim= 112,\n",
    "#     dropout= 0.5,\n",
    "#     level=1,\n",
    "#     patch_size=2,\n",
    "# )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    best_val = 0\n",
    "    os.makedirs(os.path.dirname(CKPT_SAVE_PATH), exist_ok=True)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training phase\n",
    "        for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            images = images.transpose(-1, -2)\n",
    "#             plt.imshow(images[0].squeeze(0).squeeze(0), cmap='gray')\n",
    "#             plt.title(class_map[str(labels[0].item() - 1)])\n",
    "#             plt.show()\n",
    "#             return\n",
    "            # if isinstance(model, WaveMix):\n",
    "            #     images = images.repeat(1, 3, 1, 1)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            for val_images, val_labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "                val_images = val_images.transpose(-1, -2)\n",
    "                # if isinstance(model, WaveMix):\n",
    "                #     val_images = val_images.repeat(1, 3, 1, 1)\n",
    "                val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "                val_outputs = model(val_images)  # Forward pass\n",
    "                val_loss = criterion(val_outputs, val_labels)  # Calculate validation loss\n",
    "                \n",
    "                running_val_loss += val_loss.item()  # Accumulate validation loss\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(val_outputs.data, 1)  # Get the index of the max log-probability\n",
    "                total += val_labels.size(0)  # Total number of labels\n",
    "                correct += (predicted == val_labels).sum().item()  # Count correct predictions\n",
    "\n",
    "        # Calculate average validation loss and accuracy\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        accuracy = correct / total * 100  # Percentage\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        if accuracy > best_val:\n",
    "            torch.save(model.state_dict(), CKPT_SAVE_PATH)\n",
    "            print(f\"Better model Accuracy saved\")\n",
    "\n",
    "    print(\"Training and validation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:15<00:00, 25.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.5290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:07<00:00, 45.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Validation Loss: 0.2932, Accuracy: 90.11%\n",
      "Better model Accuracy saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:14<00:00, 26.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss: 0.2479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:08<00:00, 37.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Validation Loss: 0.2354, Accuracy: 92.06%\n",
      "Better model Accuracy saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:26<00:00, 22.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Training Loss: 0.2099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:07<00:00, 43.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Validation Loss: 0.2207, Accuracy: 92.76%\n",
      "Better model Accuracy saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:24<00:00, 23.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Training Loss: 0.1885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:06<00:00, 47.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Validation Loss: 0.1993, Accuracy: 93.40%\n",
      "Better model Accuracy saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:23<00:00, 23.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Training Loss: 0.1724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:07<00:00, 41.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Validation Loss: 0.1993, Accuracy: 93.43%\n",
      "Better model Accuracy saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:26<00:00, 22.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Training Loss: 0.1608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:07<00:00, 40.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Validation Loss: 0.1952, Accuracy: 93.56%\n",
      "Better model Accuracy saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:24<00:00, 23.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Training Loss: 0.1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:07<00:00, 41.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Validation Loss: 0.1961, Accuracy: 93.71%\n",
      "Better model Accuracy saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:28<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Training Loss: 0.1409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:07<00:00, 42.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Validation Loss: 0.1994, Accuracy: 93.81%\n",
      "Better model Accuracy saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:25<00:00, 22.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Training Loss: 0.1339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:07<00:00, 41.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Validation Loss: 0.1915, Accuracy: 93.91%\n",
      "Better model Accuracy saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:20<00:00, 24.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Training Loss: 0.1263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:06<00:00, 51.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Validation Loss: 0.2093, Accuracy: 93.35%\n",
      "Better model Accuracy saved\n",
      "Training and validation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below has early stopping run the below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train():\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss as infinity\n",
    "    epochs_since_improvement = 0  # Counter to track epochs without improvement\n",
    "    patience = 5  # Number of epochs with no improvement after which training will stop\n",
    "    os.makedirs(os.path.dirname(CKPT_SAVE_PATH), exist_ok=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training phase\n",
    "        for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            images = images.transpose(-1, -2)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            for val_images, val_labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "                val_images = val_images.transpose(-1, -2)\n",
    "                val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "                val_outputs = model(val_images)  # Forward pass\n",
    "                val_loss = criterion(val_outputs, val_labels)  # Calculate validation loss\n",
    "                \n",
    "                running_val_loss += val_loss.item()  # Accumulate validation loss\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(val_outputs.data, 1)  # Get the index of the max log-probability\n",
    "                total += val_labels.size(0)  # Total number of labels\n",
    "                correct += (predicted == val_labels).sum().item()  # Count correct predictions\n",
    "\n",
    "        # Calculate average validation loss and accuracy\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        accuracy = correct / total * 100  # Percentage\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss  # Update best validation loss\n",
    "            epochs_since_improvement = 0  # Reset counter if improvement is seen\n",
    "            torch.save(model.state_dict(), CKPT_SAVE_PATH)  # Save the best model\n",
    "            print(f\"Improved validation loss! Model saved.\")\n",
    "        else:\n",
    "            epochs_since_improvement += 1  # Increment counter if no improvement\n",
    "        \n",
    "        # If no improvement for 'patience' epochs, stop training\n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(f\"Early stopping triggered. Validation loss did not improve for {patience} epochs.\")\n",
    "            break  # Exit the training loop\n",
    "\n",
    "    print(\"Training and validation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:13<00:00, 26.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.1215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:06<00:00, 48.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Validation Loss: 0.1983, Accuracy: 93.94%\n",
      "Improved validation loss! Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:12<00:00, 26.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss: 0.1155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:06<00:00, 48.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Validation Loss: 0.2048, Accuracy: 93.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:17<00:00, 25.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Training Loss: 0.1096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:07<00:00, 42.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Validation Loss: 0.2218, Accuracy: 93.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:18<00:00, 24.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Training Loss: 0.1046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:08<00:00, 38.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Validation Loss: 0.2162, Accuracy: 93.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:25<00:00, 22.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Training Loss: 0.1014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:08<00:00, 38.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Validation Loss: 0.2200, Accuracy: 93.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1950/1950 [01:25<00:00, 22.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Training Loss: 0.0975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 325/325 [00:08<00:00, 38.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Validation Loss: 0.2290, Accuracy: 93.75%\n",
      "Early stopping triggered. Validation loss did not improve for 5 epochs.\n",
      "Training and validation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "af9d54efc1bf4226517c43df189ef1481fbf7b480d9fa1a27c218cc4355c0e8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
